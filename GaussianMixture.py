# -*- coding: utf-8 -*-
"""Copy of Gaussian Mixture Models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1emRhaJp-H6Y1y6-0Piz6vPUFgsdx8okg

#  Gaussian Mixture Models for generating new faces

Training a Gaussian mixture model on the Olivetti faces dataset.

First get the olivetti data sets.
"""

from sklearn.datasets import fetch_olivetti_faces

olivetti = fetch_olivetti_faces()

print(olivetti.DESCR)

olivetti.target

"""## 1. Train a Gaussian mixture model on the Olivetti faces dataset. If you are training on your local system, you can use the dataset as is. But if you are using Google Colab, you will have to reduce your data, otherwise, your RAM will crash. For now, we will provide you with the code for PCA dimensionality reduction, which you will learn what it does in the following weeks. You can use the provided code for that part.

Step 1: Shuffle and split the data to have a better distribution using StratifiedShuffleSplit. Split the data to train set, test set and validation set.
"""

from sklearn.model_selection import StratifiedShuffleSplit

strat_split = StratifiedShuffleSplit(n_splits=1, test_size=40, random_state=42)
train_valid_idx, test_idx = next(strat_split.split(olivetti.data,
                                                   olivetti.target))
X_train_valid = olivetti.data[train_valid_idx]
y_train_valid = olivetti.target[train_valid_idx]
X_test = olivetti.data[test_idx]
y_test = olivetti.target[test_idx]

strat_split = StratifiedShuffleSplit(n_splits=1, test_size=80, random_state=43)
train_idx, valid_idx = next(strat_split.split(X_train_valid, y_train_valid))
X_train = X_train_valid[train_idx]
y_train = y_train_valid[train_idx]
X_valid = X_train_valid[valid_idx]
y_valid = y_train_valid[valid_idx]

print(X_train.shape, y_train.shape)
print(X_valid.shape, y_valid.shape)
print(X_test.shape, y_test.shape)

"""this part is to avoid overflow of your RAM, ignore it for now. you will learn more about them on next lecture."""

from sklearn.decomposition import PCA

pca = PCA(0.97)
X_train_reduced = pca.fit_transform(X_train)
X_valid_reduced = pca.transform(X_valid)
X_test_reduced = pca.transform(X_test)

"""Using the GaussianMixture of sklearn, and with 20 number of components, train a gaussian mixture model on your data.

(set your random state to 42 for reproducibility and you can use the X_train_reduced from the previous part)

"""

from sklearn.mixture import GaussianMixture

gm = GaussianMixture(n_components=20, random_state=42)
gm.fit(X_train_reduced)
y_pred = gm.predict(X_valid_reduced)

"""## 2. Generate 30 new faces by sampling from your model and visualize them."""

n_generated_faces = 30
generated_faces_reduced_format, y_gen_faces = gm.sample(n_generated_faces)

"""Now just run this line and ignore what it does as well:D
since your results are in reduced format, you have to use the inverse transform.
"""

generated_faces = pca.inverse_transform(generated_faces_reduced_format)

"""plot your generated faces. you can use the provided function for it."""

import matplotlib.pyplot as plt

def plot_faces(faces, labels, n_cols=5):
    faces = faces.reshape(-1, 64, 64)
    n_rows = (len(faces) - 1) // n_cols + 1
    plt.figure(figsize=(n_cols, n_rows * 1.1))
    for index, (face, label) in enumerate(zip(faces, labels)):
        plt.subplot(n_rows, n_cols, index + 1)
        plt.imshow(face, cmap="gray")
        plt.axis("off")
        plt.title(label)
    plt.show()

plot_faces(generated_faces, y_gen_faces)

"""## 3. Take the first 5 images of your dataset and create 15 changed images. The first five(changed) images must be rotated 90 degrees counterclockwise. The second five (changed) images must beflipped. The third five images must be darker than the original images (multiply their channels by 0.3.)

this always takes the first 5?????
"""

import numpy as np
n_rotated = 5
rotated = np.array([np.rot90(X_train[i].reshape(64, 64)) for i in range(n_rotated)])
rotated = rotated.reshape(-1, 64*64)
y_rotated = y_train[:n_rotated]

n_flipped = 5
flipped = np.array([np.flipud(X_train[i].reshape(64, 64)) for i in range(n_flipped)])
flipped = flipped.reshape(-1, 64*64)
y_flipped = y_train[:n_flipped]

n_darkened = 5
darkened = X_train[:n_darkened].copy()
darkened *= 0.3
y_darkened = y_train[:n_darkened]

X_bad_faces = np.r_[rotated, flipped, darkened]
y_bad = np.concatenate([y_rotated, y_flipped, y_darkened])

plot_faces(X_bad_faces, y_bad)

"""## 4. Compute the log-likelihood of each sample. (you can use the gm methods.) Compare these log-likelihoods with the main data scores (maybe for the first 15 samples). Explain the results and describe how we use GMMs for anomaly detection.

ignore this part for now:D
"""

X_bad_faces_pca = pca.transform(X_bad_faces)

# --- fill here

# --- fill here