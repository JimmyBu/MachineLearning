# -*- coding: utf-8 -*-
"""Copy of Discount factor effect and Q-Learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FvMTFBoITFaoOK7xzUG_1_reAwfayuQX

# Discount factor effect and Q-Learning

## 1. Based on the probabilities on the arrows above, Model your MDP (transition probabilities and rewards) in the notebook. you can use the [s, a, sâ€™] for each part or you can use your own way of defining the MDP. What should be seen are transition probabilities, rewards and possible actions
"""

transition_probabilities ={
    's0': {
        'a0': [(0.7, 's0'), (0.3, 's1')],
        'a1': [(1, 's0')],
        'a2':[(0.8, 's0'), (0.2, 's1')]
    },
    's1':{
        'a0': [(1, 's1')],
        'a2': [(1, 's2')]
    },
    's2':{
        'a1': [(0.1, 's2'), (0.1, 's1'), (0.8, 's0')]
    }
}
rewards = {
    ('s0', 'a0', 's0') : 10,
    ('s0', 'a0', 's1') : 0,
    ('s0', 'a1', 's0') : 0,
    ('s0', 'a2', 's0') : 0,
    ('s0', 'a2', 's1') : 0,
    ('s1', 'a0', 's1') : 0,
    ('s1', 'a2', 's2') : -50,
    ('s2', 'a1', 's2') : 0,
    ('s2', 'a1', 's1') : 0,
    ('s2', 'a1', 's0') : 40,
}
actions = {
    'a0',
    'a1',
    'a2'
}

possible_actions = {
    's0': ['a0', 'a1', 'a2'],
    's1': ['a0', 'a2'],
    's2': ['a1']
}

states = ['s0', 's1', 's2']

import numpy as np
import random

"""Are you sure enumerate is correctly used here?"""

Q = {}
for state in states:
    for action in possible_actions[state]:
        Q[(state, action)] = 0.0

Q  # dont understand why 9, should be # of actions? 6?

gamma = 0.9
# epsilon = 1 # 60% random action, 40% highest Q-value
# alpha = 1  # learning rate
# max_steps = 100

for iteration in range(50):  # epochs = 50
    # print(state)
    for state in states:
        for action in possible_actions[state]:
            for val in transition_probabilities[state][action]:
                max_q = 0.0
                for a in possible_actions[val[1]]:
                    if Q[(val[1], a)] in Q:
                        max_q = max(max_q, Q[(val[1], a)])
                Q[(val[1], action)] = rewards[(state, action, val[1])] * gamma * val[0] + max_q

optimal_policy = {}
for state in states:
    optimal_policy[state] = max(possible_actions[state], key=lambda a: Q[(state, a)])

optimal_policy



Q_values = np.full((3, 3), -np.inf)  # -np.inf for impossible actions
for state, actions in enumerate(possible_actions):
    print(state, actions)
    Q_values[(state, actions)] = 0.0  # for all possible actions

"""## 2. Take your discount factor to be 0.9. Perform Q-learning and report the Q-values for each (state, action) pair. Based on that, what is the optimal policy?"""

gamma = 0.9  # the discount factor
# gamma = 0.95
for iteration in range(50):
    # --- fill here (perform a DP approach for filling up your Q-table (repeat the process by stting gamma to be 0.95 )

Q_values

Q_values.argmax(axis=1)  # optimal action for each state

"""## 3. Perform the same procedure but this time with a discount factor of 0.95. Did your optimal policy change? Explain your results.


"""

# you can use the same code as above

import numpy as np

# Define the states and actions
states = ['s0', 's1', 's2']
actions = ['a0', 'a1', 'a2']

# Initialize Q-table with -np.inf
Q_values = np.full((len(states), len(actions)), -np.inf)

# Mapping states and actions to indices
state_index = {state: idx for idx, state in enumerate(states)}
action_index = {action: idx for idx, action in enumerate(actions)}

# Define rewards
rewards = {
    ('s0', 'a0', 's0') : 10,
    ('s0', 'a0', 's1') : 0,
    ('s0', 'a1', 's0') : 0,
    ('s0', 'a2', 's0') : 0,
    ('s0', 'a2', 's1') : 0,
    ('s1', 'a0', 's1') : 0,
    ('s1', 'a2', 's2') : -50,
    ('s2', 'a1', 's2') : 0,
    ('s2', 'a1', 's1') : 0,
    ('s2', 'a1', 's0') : 40,
}

# Update Q-values with the given rewards
for (state, action, _), reward in rewards.items():
    state_idx = state_index[state]
    action_idx = action_index[action]
    Q_values[state_idx, action_idx] = 0.0

print(Q_values)

num_states = len(states)
num_actions = len(actions)
transition_prob_array = np.zeros((num_states, num_actions, num_states), dtype=float)

for state, actions_dict in transition_probabilities.items():
    for action, transitions in actions_dict.items():
        state_idx = state_index[state]
        action_idx = action_index[action]
        for prob, next_state in transitions:
            next_state_idx = state_index[next_state]
            transition_prob_array[state_idx, action_idx, next_state_idx] = prob

print("Transition Probabilities Array:")
print(transition_prob_array)

gamma = 0.9

for iteration in range(50):
    # Create a copy of Q_values to avoid in-place updates
    Q_values_copy = np.copy(Q_values)

    for state in states:
        for action in possible_actions[state]:
            action_idx = action_index[action]
            max_q = float('-inf')  # Initialize to negative infinity
            next_state_probs = transition_probabilities[state][action]

            # Calculate the updated Q-value for the state-action pair
            for prob, next_state in next_state_probs:
                next_state_idx = state_index[next_state]

                # Find the maximum Q-value for the next state
                max_q_next = float('-inf')
                for next_action in possible_actions[next_state]:
                    next_action_idx = action_index[next_action]
                    max_q_next = max(max_q_next, Q_values_copy[next_state_idx, next_action_idx])

                # Update Q-value using the Bellman equation
                new_q_value = rewards.get((state, action, next_state), 0) + gamma * prob * max_q_next

                # Only update if the new Q-value is greater
                Q_values[state_index[state], action_idx] = max(Q_values[state_index[state], action_idx], new_q_value)

Q_values



gamma = 0.9
for iteration in range(50):
  Q_values_copy = np.copy(Q_values)
  for s in range(3):
    for a in possible_actions[s]:
      max_q = []
      for sp in range(3):
        rewards = rewards[s][a][sp]
        transition_prob = transition_probabilities[s][a][sp]
        max_q.append(transition_prob * (rewards + gamma * max(Q_values_copy[sp])))
      Q_values[s][a] = np.sum(max_q)

import numpy as np

transition_probabilities = {
    's0': {
        'a0': [(0.7, 's0'), (0.3, 's1')],
        'a1': [(1, 's0')],
        'a2': [(0.8, 's0'), (0.2, 's1')]
    },
    's1': {
        'a0': [(1, 's1')],
        'a2': [(1, 's2')]
    },
    's2': {
        'a1': [(0.1, 's2'), (0.1, 's1'), (0.8, 's0')]
    }
}
rewards = {
    ('s0', 'a0', 's0'): 10,
    ('s0', 'a0', 's1'): 0,
    ('s0', 'a1', 's0'): 0,
    ('s0', 'a2', 's0'): 0,
    ('s0', 'a2', 's1'): 0,
    ('s1', 'a0', 's1'): 0,
    ('s1', 'a2', 's2'): -50,
    ('s2', 'a1', 's2'): 0,
    ('s2', 'a1', 's1'): 0,
    ('s2', 'a1', 's0'): 40,
}
possible_actions = {
    's0': ['a0', 'a1', 'a2'],
    's1': ['a0', 'a2'],
    's2': ['a1']
}

# Initialize Q-values
states = ['s0', 's1', 's2']
actions = ['a0', 'a1', 'a2']
Q_values = np.full((3, 3), -np.inf)

# Set initial Q-values to 0 for valid actions
for state_idx, state in enumerate(states):
    for action in possible_actions[state]:
        action_idx = actions.index(action)
        Q_values[state_idx, action_idx] = 0.0

gamma = 0.95
for iteration in range(50):
    Q_values_copy = np.copy(Q_values)
    for s_idx, state in enumerate(states):
        for a in possible_actions[state]:
            a_idx = actions.index(a)
            max_q = 0
            for prob, next_state in transition_probabilities[state][a]:
                sp_idx = states.index(next_state)
                reward = rewards.get((state, a, next_state), 0)
                max_q += prob * (reward + gamma * np.max(Q_values_copy[sp_idx]))
            Q_values[s_idx, a_idx] = max_q

Q_values

Q_values.argmax(axis=1)

