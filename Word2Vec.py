# -*- coding: utf-8 -*-
"""Copy of Word2Vec_Skeleton.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B4KvlI1ni2flLRlkgTbxL6DOXdhsNvf2

# Importing libraries
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import os
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Dense, Lambda
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
import tensorflow.keras.backend as K

from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

import random

"""# Set random seed"""

# Set environment variables
os.environ['PYTHONHASHSEED'] = str(25)
os.environ['TF_DETERMINISTIC_OPS'] = '1'
os.environ['TF_CUDNN_DETERMINISTIC'] = '1'

# Set seed values
np.random.seed(25)
tf.random.set_seed(25)
random.seed(25)

"""# TODO: Preprocess"""

# Preprocess the text
def preprocess(text):
    # TODO
    text = text.lower()
    words = text.split()
    return words

text_path = "/content/drive/My Drive/small_corpus.txt"
with open(text_path, 'r') as file:
    text = file.read()

print(len(preprocess(text)))

"""# TODO: Build Vocabulary and training data"""

# TODO: Build vocabulary and generate training data
def build_and_prepare_data(words, window_size=2):
    # Build vocabulary
    vocab = {}
    index = 0
    for word in words:
        if word not in vocab:
            vocab[word] = index
            index += 1
    # Generate context-target pairs
    contexts = []
    targets = []
    for i in range(window_size, len(words) - window_size):
        context = []
        for j in range(-window_size, window_size + 1):
            if j != 0:
                context.append(vocab[words[i + j]])
        contexts.append(context)
        targets.append(vocab[words[i]])

    # Extract contexts and targets from data
    contexts = np.array(contexts)
    targets = np.array(targets)

    # Prepare contexts and targets for training by padding and one-hot encoding
    targets = to_categorical(targets, num_classes=len(vocab))

    return vocab, contexts, targets

"""# TODO: Build CBOW model"""

# Define CBOW model function
def build_cbow_model(vocab_size, window_size, embed_size=2):
    # TODO
    # Input
    input_layer = Input(shape=(2 * window_size,), dtype='int32', name='context_words')
    # Embedding
    embedding_layer = Embedding(input_dim=vocab_size, output_dim=embed_size, name='embedding')(input_layer)
    # Mean
    avg_embeddings = Lambda(lambda x: K.mean(x, axis=1), name='mean_embedding')(embedding_layer)
    # output
    output_layer = Dense(vocab_size, activation='softmax', name='output')(avg_embeddings)
    # model
    model = Model(inputs=[input_layer], outputs=[output_layer])
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

    return model

"""# TODO: Set file path"""

# TODO: set correct file path
file_path = '/content/drive/My Drive/small_corpus.txt'

"""# Running the helper functions"""

# Read the file
with open(file_path, 'r', encoding='utf-8') as file:
    text = file.read()

words = preprocess(text)

# Print vocabulary size
print(f"Number of words: {len(words)}")

# Model parameters
window_size = 2

# Prepare dataset
vocab, contexts, targets = build_and_prepare_data(words, window_size)

vocab_size = len(vocab)
# Print vocabulary size
print(f"Vocabulary size: {vocab_size}")

# Print lengths of contexts and targets
print(f"Length of contexts array: {len(contexts)}")
print(f"Length of targets array: {len(targets)}")

vocab, contexts, targets

"""# Split the data inton training and validation sets"""

# Splitting the data
contexts_train, contexts_val, targets_train, targets_val = train_test_split(contexts, targets, test_size=0.2, random_state=25)

embed_size = 2

"""# Train the model"""

# Create and train the model
model = build_cbow_model(vocab_size, window_size, embed_size)
history = model.fit(contexts_train, targets_train, validation_data=(contexts_val, targets_val), epochs=7, verbose=1)

model.summary()

"""# TODO: Visualise the Training and Validation loss"""

# Plotting the training and validation loss
#TODO
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""# TODO: Extract the embeddings"""

# Extract embeddings
#TODO
embedding_layer = model.get_layer('embedding')
embeddings = embedding_layer.get_weights()[0]
embeddings

"""# TODO: Find similar words"""

def cosine_similarity(vec_a, vec_b):
    """Calculate the cosine similarity between two vectors."""
    #TODO
    dot_product = np.dot(vec_a, vec_b)
    norm_a = np.linalg.norm(vec_a)
    norm_b = np.linalg.norm(vec_b)
    similarity = dot_product / (norm_a * norm_b)
    return similarity

def find_similar_words(query_word, vocab, embeddings, top_n=3):
    """Find the top_n words most similar to the query_word based on the embeddings."""
    similarities = []

    #TODO populate the similarities list
    query_index = vocab[query_word]
    query_embedding = embeddings[query_index]
    for word, index in vocab.items():
        if word != query_word:
            sim = cosine_similarity(query_embedding, embeddings[index])
            similarities.append((word, sim))

    # Sort based on similarity scores
    similarities.sort(key=lambda x: x[1], reverse=True)

    # Sort based on similarity scores
    similarities.sort(key=lambda x: x[1], reverse=True)

    # Print top similar words
    print(f"Words most similar to '{query_word}':")
    for word, similarity in similarities[:top_n]:
        print(f"{word}: {similarity:.4f}")

query_words = ['poland', 'thailand', 'morocco']

for query_word in query_words:
    find_similar_words(query_word, vocab, embeddings)
    print("\n")

"""# TODO: Visualise the embeddings"""

# Create a scatter plot of the embeddings
# TODO
plt.figure(figsize=(10, 10))
for word, index in vocab.items():
    x, y = embeddings[index]
    plt.scatter(x, y)
    plt.text(x+0.01, y+0.01, word, fontsize=9)

plt.title('Similarity')
plt.show()