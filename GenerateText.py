# -*- coding: utf-8 -*-
"""Copy of LSTM_Skeleton.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yIOlQ00-ej9v351FTIAqItCxuP3qjFrM

# TODO: Import the required libraries
"""

from google.colab import drive
drive.mount('/content/drive')

# TODO
import os
import re
import numpy as np
import tensorflow as tf
import random
import keras.backend as K
from keras.layers import Layer
from keras.models import Model
from tensorflow.keras.preprocessing.text import Tokenizer
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, GRU, Dense, LayerNormalization, Dropout, BatchNormalization, SimpleRNN
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping

"""# Adding random seed"""

# Set environment variables
os.environ['PYTHONHASHSEED'] = str(25)
os.environ['TF_DETERMINISTIC_OPS'] = '1'
os.environ['TF_CUDNN_DETERMINISTIC'] = '1'

# Set seed values
np.random.seed(25)
tf.random.set_seed(25)
random.seed(25)

"""# TODO: Read and Preprocess the dataset"""

path = "/content/drive/My Drive/alice.txt"
text = ""

# TODO: Load and preprocess the text
# Read the file
with open(path, 'r', encoding='utf-8') as file:
    text = file.read()

# Convert to lowercase
text = text.lower()

# Remove punctuations
text = re.sub(r'[^\w\s]', '', text)

print(len(text))

"""# TODO: Using tokenizers"""

# TODO: Tokenize the text
tokenizer = Tokenizer()
tokenizer.fit_on_texts([text])

words = tokenizer.word_index
total_words = len(words) + 1

print(total_words)

"""# TODO: Feature Engineering"""

# TODO: Create input sequences
input_sequences = []

# newline
lines = text.split('\n')

for line in lines:
    tokens = tokenizer.texts_to_sequences([line])[0]
    for i in range(1, len(tokens)):
        n_gram_sequence = tokens[:i+1]
        input_sequences.append(n_gram_sequence)

# TODO: Pad sequences
max_sequence_len = max([len(seq) for seq in input_sequences])
input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')

print(len(input_sequences))

"""# TODO: Storing features and labels"""

# TODO: Create predictors and labels
predictors, labels = input_sequences[:, :-1], input_sequences[:, -1]
labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)

# TODO: Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(predictors, labels, test_size=0.2, random_state=25)

print(f"X_train: {X_train.shape}")
print(f"X_val: {X_val.shape}")
print(f"y_train: {y_train.shape}")
print(f"y_val: {y_val.shape}")

"""# TODO: Building our model"""

# TODO: Build your model
def build_lstm_model(input_dim, output_dim, input_length):
    model = Sequential()
    model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))
    model.add(LSTM(150))
    model.add(Dense(input_dim, activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

input_dim = total_words
output_dim = 100
input_length = max_sequence_len - 1

lstm_model = build_lstm_model(input_dim, output_dim, input_length)
lstm_model.summary()

"""# TODO: Model training"""

# TODO: Train your model
history = lstm_model.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val), verbose=1)

"""# TODO: Visualising the Training and Validation Accuracies and Losses against the number of Epochs"""

# TODO: Plotting the training and validation loss and accuracy
plt.figure(figsize=(12, 4))
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

class Attention(Layer):
    def __init__(self, **kwargs):
        super(Attention, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(name='attention_weight',
                                 shape=(input_shape[-1], 1),
                                 initializer='random_normal',
                                 trainable=True)
        self.b = self.add_weight(name='attention_bias',
                                 shape=(input_shape[1], 1),
                                 initializer='zeros',
                                 trainable=True)
        super(Attention, self).build(input_shape)

    def call(self, x):
        # Alignment scores
        e = K.tanh(K.dot(x, self.W) + self.b)
        # Remove dimension of size 1
        e = K.squeeze(e, axis=-1)
        # weights
        alpha = K.softmax(e)
        # Reshape
        alpha = K.expand_dims(alpha, axis=-1)
        # vectorize
        context = x * alpha
        return context

def build_my_model(input_dim, output_dim, input_length):
    model = Sequential()
    model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))

    model.add(Bidirectional(LSTM(100, return_sequences=True, kernel_regularizer=l2(0.001))))
    model.add(Dropout(0.5))
    model.add(BatchNormalization())

    model.add(SimpleRNN(10, return_sequences=True, activation='tanh'))
    model.add(Attention())
    model.add(BatchNormalization())

    model.add(GRU(200, return_sequences=False, kernel_regularizer=l2(0.001)))
    model.add(Dropout(0.5))
    model.add(BatchNormalization())

    model.add(Dense(256, activation='tanh', kernel_regularizer=l2(0.001)))
    model.add(Dropout(0.5))
    model.add(Dense(128, activation='tanh', kernel_regularizer=l2(0.001)))
    model.add(Dense(input_dim, activation='softmax'))

    adam = Adam(learning_rate=0.001, clipvalue=1.0)
    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])

    return model

input_dim = total_words
output_dim = 100
input_length = max_sequence_len - 1

my_model = build_my_model(input_dim, output_dim, input_length)
my_model.summary()

early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.00001)

improved_history = my_model.fit(X_train, y_train, epochs = 20, validation_data=(X_val, y_val), verbose=1)

plt.figure(figsize=(12, 4))
plt.plot(improved_history.history['accuracy'], label='Train Accuracy')
plt.plot(improved_history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Improved Model Accuracy')
plt.legend()
plt.show()

plt.plot(improved_history.history['loss'], label='Train Loss')
plt.plot(improved_history.history['val_loss'], label='Validation Loss')
plt.title('Improved Model Loss')
plt.legend()
plt.show()

"""# TODO: Generate text"""

def generate_text(start_text, num_generate, logits_model, max_sequence_len, tokenizer, temperature=1.0):
    generated_text = start_text.strip().lower()
    input_sequence = tokenizer.texts_to_sequences([start_text])[0]

    for _ in range(num_generate):
        padded_sequence = pad_sequences([input_sequence], maxlen=max_sequence_len-1, padding='pre')

        logits = logits_model.predict(padded_sequence, verbose=0)[0]

        logits = logits / temperature

        # apply softmax after
        predictions = tf.nn.softmax(logits).numpy()

        predicted_id = np.random.choice(len(predictions), p=predictions)
        predicted_word = tokenizer.index_word.get(predicted_id, "")
        generated_text += " " + predicted_word

        input_sequence.append(predicted_id)
        input_sequence = input_sequence[1:]

    return generated_text

seed_text = "Forest is"
next_words = 10
temperature_low = 0.05
temperature_high = 1.5

my_temp_model = Model(inputs=lstm_model.input, outputs=lstm_model.layers[-2].output)

print("Generated text with temperature 0.05:")
generated_text_low = generate_text(seed_text, next_words, my_temp_model, max_sequence_len, tokenizer, temperature=temperature_low)
print(generated_text_low)

print("\nGenerated text with temperature 1.5:")
generated_text_high = generate_text(seed_text, next_words, my_temp_model, max_sequence_len, tokenizer, temperature=temperature_high)
print(generated_text_high)